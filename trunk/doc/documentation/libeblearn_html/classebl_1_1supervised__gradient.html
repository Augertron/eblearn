<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
<title>libeblearn: ebl::supervised_gradient Class Reference</title>
<link href="doxygen.css" rel="stylesheet" type="text/css">
<link href="tabs.css" rel="stylesheet" type="text/css">
</head><body>
<!-- Generated by Doxygen 1.5.6 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul>
      <li><a href="main.html"><span>Main&nbsp;Page</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li><a href="examples.html"><span>Examples</span></a></li>
    </ul>
  </div>
  <div class="tabs">
    <ul>
      <li><a href="annotated.html"><span>Class&nbsp;List</span></a></li>
      <li><a href="hierarchy.html"><span>Class&nbsp;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&nbsp;Members</span></a></li>
    </ul>
  </div>
  <div class="navpath"><b>ebl</b>::<a class="el" href="classebl_1_1supervised__gradient.html">supervised_gradient</a>
  </div>
</div>
<div class="contents">
<h1>ebl::supervised_gradient Class Reference</h1><!-- doxytag: class="ebl::supervised_gradient" --><!-- doxytag: inherits="ebl::supervised" --><code>#include &lt;<a class="el" href="Trainer_8h-source.html">Trainer.h</a>&gt;</code>
<p>
<div class="dynheader">
Inheritance diagram for ebl::supervised_gradient:</div>
<div class="dynsection">

<p><center><img src="classebl_1_1supervised__gradient.png" usemap="#ebl::supervised_gradient_map" border="0" alt=""></center>
<map name="ebl::supervised_gradient_map">
<area href="classebl_1_1supervised.html" alt="ebl::supervised" shape="rect" coords="0,56,150,80">
<area href="classebl_1_1eb__trainer.html" alt="ebl::eb_trainer" shape="rect" coords="0,0,150,24">
</map>
</div>

<p>
<a href="classebl_1_1supervised__gradient-members.html">List of all members.</a><table border="0" cellpadding="0" cellspacing="0">
<tr><td></td></tr>
<tr><td colspan="2"><br><h2>Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" nowrap align="right" valign="top">&nbsp;</td><td class="memItemRight" valign="bottom"><a class="el" href="classebl_1_1supervised__gradient.html#55ee22b889791fa22fd09e4c8681f227">supervised_gradient</a> (<a class="el" href="classebl_1_1idx3__supervised__module.html">idx3_supervised_module</a> *m, <a class="el" href="classebl_1_1parameter.html">parameter</a> *p, <a class="el" href="classebl_1_1state__idx.html">state_idx</a> *e=NULL, <a class="el" href="classebl_1_1state__idx.html">state_idx</a> *in=NULL, <a class="el" href="classebl_1_1class__state.html">class_state</a> *out=NULL, Idx&lt; ubyte &gt; *des=NULL)</td></tr>

<tr><td class="memTemplParams" nowrap colspan="2">template&lt;class T, class L&gt; </td></tr>
<tr><td class="memTemplItemLeft" nowrap align="right" valign="top">void&nbsp;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classebl_1_1supervised__gradient.html#d7c35edfa14df868f60b30f56088116d">train_online</a> (LabeledDataSource&lt; T, L &gt; *ds, <a class="el" href="classebl_1_1classifier__meter.html">classifier_meter</a> *mtr, intg n, <a class="el" href="classebl_1_1gd__param.html">gd_param</a> *gdp, double kappa)</td></tr>

<tr><td class="memTemplParams" nowrap colspan="2">template&lt;class T, class L&gt; </td></tr>
<tr><td class="memTemplItemLeft" nowrap align="right" valign="top">void&nbsp;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classebl_1_1supervised__gradient.html#ebac5003cf4dda19b65276066d99bb73">train</a> (LabeledDataSource&lt; T, L &gt; *ds, <a class="el" href="classebl_1_1classifier__meter.html">classifier_meter</a> *mtr, <a class="el" href="classebl_1_1gd__param.html">gd_param</a> *gdp, double kappa=0.0)</td></tr>

<tr><td class="memTemplParams" nowrap colspan="2">template&lt;class T, class L&gt; </td></tr>
<tr><td class="memTemplItemLeft" nowrap align="right" valign="top">void&nbsp;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classebl_1_1supervised__gradient.html#2c715d95b245c458c4987320782f0a9b">compute_diaghessian</a> (LabeledDataSource&lt; T, L &gt; *ds, intg n, double mu)</td></tr>

<tr><td class="memTemplParams" nowrap colspan="2">template&lt;class T, class L&gt; </td></tr>
<tr><td class="memTemplItemLeft" nowrap align="right" valign="top">void&nbsp;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classebl_1_1supervised__gradient.html#368de546ba568fd7655c1034cb003598">saliencies</a> (LabeledDataSource&lt; T, L &gt; *ds, intg n)</td></tr>

</table>
<hr><a name="_details"></a><h2>Detailed Description</h2>
A basic trainer object for <a class="el" href="classebl_1_1supervised.html">supervised</a> stochastic gradient training of a classifier with discrete class labels. This is a subclass of &lt;supervised&gt;. The machine's fprop method must have four arguments: input, output, energy, and desired output. A call to the machine's fprop must look like this: {<code> (==&gt; machine fprop input output desired energy) </code>} where &lt;output&gt; must be a &lt;class-state&gt;, &lt;desired&gt; an idx0 of int (integer scalar), and &lt;energy&gt; and &lt;idx0-ddstate&gt;. The meter passed to the training and testing methods should be a &lt;classifier-meter&gt;, or any meter whose update method looks like this: {<code> (==&gt; meter update output desired energy) </code>} where &lt;output&gt; must be a &lt;class-state&gt;, &lt;desired&gt; an idx0 of int, and &lt;energy&gt; and &lt;idx0-ddstate&gt;. The trainable <a class="el" href="classebl_1_1parameter.html">parameter</a> object must understand the following methods: {<ul>
</ul>
{<code> (==&gt; param clear-dx)}: clear the gradients.} { {<code> (==&gt; param update eta inertia)}: update the parameters with learning rate &lt;eta&gt;, and momentum term &lt;inertia&gt;.} } If the diagonal hessian estimation is to be used, the param object must also understand: {<ul>
</ul>
{<code> (==&gt; param clear-ddx)}: clear the second derivatives.} { {<code> (==&gt; param update-ddeltas knew kold)}: update average second derivatives.} { {<code> (==&gt; param compute-epsilons &lt;mu&gt;)}: set the per-parameter learning rates to the inverse of the sum of the second derivative estimates and &lt;mu&gt;.} } </code></code></code></code></code><hr><h2>Constructor &amp; Destructor Documentation</h2>
<a class="anchor" name="55ee22b889791fa22fd09e4c8681f227"></a><!-- doxytag: member="ebl::supervised_gradient::supervised_gradient" ref="55ee22b889791fa22fd09e4c8681f227" args="(idx3_supervised_module *m, parameter *p, state_idx *e=NULL, state_idx *in=NULL, class_state *out=NULL, Idx&lt; ubyte &gt; *des=NULL)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">ebl::supervised_gradient::supervised_gradient           </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classebl_1_1idx3__supervised__module.html">idx3_supervised_module</a> *&nbsp;</td>
          <td class="paramname"> <em>m</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classebl_1_1parameter.html">parameter</a> *&nbsp;</td>
          <td class="paramname"> <em>p</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classebl_1_1state__idx.html">state_idx</a> *&nbsp;</td>
          <td class="paramname"> <em>e</em> = <code>NULL</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classebl_1_1state__idx.html">state_idx</a> *&nbsp;</td>
          <td class="paramname"> <em>in</em> = <code>NULL</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classebl_1_1class__state.html">class_state</a> *&nbsp;</td>
          <td class="paramname"> <em>out</em> = <code>NULL</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Idx&lt; ubyte &gt; *&nbsp;</td>
          <td class="paramname"> <em>des</em> = <code>NULL</code></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
create a new &lt;supervised-gradient&gt; trainer. Arguments are as follow: {<ul>
</ul>
&lt;m&gt;: machine to be trained.} { <p>
: trainable <a class="el" href="classebl_1_1parameter.html">parameter</a> object of the machine.} { &lt;e&gt;: energy object (by default an idx0-ddstate).} { &lt;in&gt;: input object (by default an idx3-ddstate).} { &lt;out&gt;: output object (by default a class-state).} { &lt;des&gt;: desired output (by default an idx0 of int).} } 
</div>
</div><p>
<hr><h2>Member Function Documentation</h2>
<a class="anchor" name="d7c35edfa14df868f60b30f56088116d"></a><!-- doxytag: member="ebl::supervised_gradient::train_online" ref="d7c35edfa14df868f60b30f56088116d" args="(LabeledDataSource&lt; T, L &gt; *ds, classifier_meter *mtr, intg n, gd_param *gdp, double kappa)" -->
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T, class L&gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">void ebl::supervised_gradient::train_online           </td>
          <td>(</td>
          <td class="paramtype">LabeledDataSource&lt; T, L &gt; *&nbsp;</td>
          <td class="paramname"> <em>ds</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classebl_1_1classifier__meter.html">classifier_meter</a> *&nbsp;</td>
          <td class="paramname"> <em>mtr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">intg&nbsp;</td>
          <td class="paramname"> <em>n</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classebl_1_1gd__param.html">gd_param</a> *&nbsp;</td>
          <td class="paramname"> <em>gdp</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>kappa</em> = <code>0.0</code></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
train with stochastic (online) gradient on the next &lt;n&gt; samples of data source &lt;dsource&gt; with global learning rate &lt;eta&gt;. and "momentum term" &lt;inertia&gt;. Optionally maintain a running average of the weights with positive rate &lt;kappa&gt;. A negative value for kappa sets a rate equal to -&lt;kappa&gt;/&lt;age&gt;. No such update is performed if &lt;kappa&gt; is 0.<p>
Record performance in &lt;mtr&gt;. &lt;mtr&gt; must understand the following methods: {<code> (==&gt; mtr update age output desired energy) (==&gt; mtr info) </code>} where &lt;age&gt; is the number of calls to <a class="el" href="classebl_1_1parameter.html">parameter</a> updates so far, &lt;output&gt; is the machine's output (most likely a &lt;class-state&gt;), &lt;desired&gt; is the desired output (most likely an idx0 of int), and &lt;energy&gt; is an &lt;idx0-state&gt;. The &lt;info&gt; should return a list of relevant measurements. 
</div>
</div><p>
<a class="anchor" name="ebac5003cf4dda19b65276066d99bb73"></a><!-- doxytag: member="ebl::supervised_gradient::train" ref="ebac5003cf4dda19b65276066d99bb73" args="(LabeledDataSource&lt; T, L &gt; *ds, classifier_meter *mtr, gd_param *gdp, double kappa=0.0)" -->
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T, class L&gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">void ebl::supervised_gradient::train           </td>
          <td>(</td>
          <td class="paramtype">LabeledDataSource&lt; T, L &gt; *&nbsp;</td>
          <td class="paramname"> <em>ds</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classebl_1_1classifier__meter.html">classifier_meter</a> *&nbsp;</td>
          <td class="paramname"> <em>mtr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classebl_1_1gd__param.html">gd_param</a> *&nbsp;</td>
          <td class="paramname"> <em>gdp</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>kappa</em> = <code>0.0</code></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
train the machine on all the samples in data source &lt;dsource&gt; and measure the performance with &lt;mtr&gt;. 
</div>
</div><p>
<a class="anchor" name="2c715d95b245c458c4987320782f0a9b"></a><!-- doxytag: member="ebl::supervised_gradient::compute_diaghessian" ref="2c715d95b245c458c4987320782f0a9b" args="(LabeledDataSource&lt; T, L &gt; *ds, intg n, double mu)" -->
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T, class L&gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">void ebl::supervised_gradient::compute_diaghessian           </td>
          <td>(</td>
          <td class="paramtype">LabeledDataSource&lt; T, L &gt; *&nbsp;</td>
          <td class="paramname"> <em>ds</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">intg&nbsp;</td>
          <td class="paramname"> <em>n</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&nbsp;</td>
          <td class="paramname"> <em>mu</em></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Compute per-parameter learning rates (epsilons) using the stochastic diaginal levenberg marquardt method (as described in LeCun et al. "efficient backprop", available at {&lt;hlink&gt; <a href="http://yann.lecun.com">http://yann.lecun.com</a>}). This method computes positive estimates the second derivative of the objective function with respect to each <a class="el" href="classebl_1_1parameter.html">parameter</a> using the Gauss-Newton approximation. &lt;dsource&gt; is a data source, &lt;n&gt; is the number of patterns (starting at the current point in the data source) on which the estimate is to be performed. Each parameter-specific learning rate epsilon_i is computed as 1/(H_ii + mu), where H_ii are the diagonal Gauss-Newton estimates and &lt;mu&gt; is the blowup prevention fudge factor. 
</div>
</div><p>
<a class="anchor" name="368de546ba568fd7655c1034cb003598"></a><!-- doxytag: member="ebl::supervised_gradient::saliencies" ref="368de546ba568fd7655c1034cb003598" args="(LabeledDataSource&lt; T, L &gt; *ds, intg n)" -->
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T, class L&gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">void ebl::supervised_gradient::saliencies           </td>
          <td>(</td>
          <td class="paramtype">LabeledDataSource&lt; T, L &gt; *&nbsp;</td>
          <td class="paramname"> <em>ds</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">intg&nbsp;</td>
          <td class="paramname"> <em>n</em></td><td>&nbsp;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td><td><code> [inline]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>
Compute the parameters saliencies as defined in the Optimal Brain Damage algorithm of (LeCun, Denker, Solla, NIPS 1989), available at <a href="http://yann.lecun.com.">http://yann.lecun.com.</a> This computes the first and second derivatives of the energy with respect to each <a class="el" href="classebl_1_1parameter.html">parameter</a> averaged over the next &lt;n&gt; patterns of data source &lt;ds&gt;. A vector of saliencies is returned. Component <em> of the vector contains {<code> Si = -Gi * Wi + 1/2 Hii * Wi^2 }, this is an estimate of how much the energy would increase if the <a class="el" href="classebl_1_1parameter.html">parameter</a> was eliminated (set to zero). Parameters with small saliencies can be eliminated by setting their value and epsilon to zero. </code></em>
</div>
</div><p>
<hr>The documentation for this class was generated from the following files:<ul>
<li>/home/pierre/eblearn-trunk/libeblearn/include/<a class="el" href="Trainer_8h-source.html">Trainer.h</a><li>/home/pierre/eblearn-trunk/libeblearn/include/<a class="el" href="Trainer_8hpp-source.html">Trainer.hpp</a><li>/home/pierre/eblearn-trunk/libeblearn/src/Trainer.cpp</ul>
</div>
<hr size="1"><address style="text-align: right;"><small>Generated on Thu Feb 26 18:53:41 2009 for libeblearn by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img src="doxygen.png" alt="doxygen" align="middle" border="0"></a> 1.5.6 </small></address>
</body>
</html>
